{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "import requests\n",
    "import lxml\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "pd.set_option('display.max_columns', 99)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Cities to Scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this analysis, we are interested in the top 30 cities by population.  To find this information, lets scrape it from a reliable, up-to-date source found at this [Wikipedia link]('https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HEADERS = {'User-Agent':'Mozilla/5.0'}\n",
    "BASE_URL = \"https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\"\n",
    "response = requests.get(BASE_URL,headers=HEADERS)\n",
    "soup = BeautifulSoup(response.content, \"html5lib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Locate the table containing cities and populations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table = soup.select_one('table.sortable.wikitable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each row in the population table, pull the city name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "city_list = []\n",
    "for row in table.select('tr'):\n",
    "    city = row.find('a').text\n",
    "    city_list.append(city)\n",
    "#Truncate the list to limit to first 30 cities\n",
    "city_list = city_list[1:51]\n",
    "#Correct 'Washington' to 'Washington, D.C.'\n",
    "city_list[20] = \"Washington, D.C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['New York',\n",
       " 'Los Angeles',\n",
       " 'Chicago',\n",
       " 'Houston',\n",
       " 'Phoenix',\n",
       " 'Philadelphia',\n",
       " 'San Antonio',\n",
       " 'San Diego',\n",
       " 'Dallas',\n",
       " 'San Jose',\n",
       " 'Austin',\n",
       " 'Jacksonville',\n",
       " 'San Francisco',\n",
       " 'Columbus',\n",
       " 'Indianapolis',\n",
       " 'Fort Worth',\n",
       " 'Charlotte',\n",
       " 'Seattle',\n",
       " 'Denver',\n",
       " 'El Paso',\n",
       " 'Washington, D.C',\n",
       " 'Boston',\n",
       " 'Detroit',\n",
       " 'Nashville',\n",
       " 'Memphis',\n",
       " 'Portland',\n",
       " 'Oklahoma City',\n",
       " 'Las Vegas',\n",
       " 'Louisville',\n",
       " 'Baltimore',\n",
       " 'Milwaukee',\n",
       " 'Albuquerque',\n",
       " 'Tucson',\n",
       " 'Fresno',\n",
       " 'Sacramento',\n",
       " 'Mesa',\n",
       " 'Kansas City',\n",
       " 'Atlanta',\n",
       " 'Long Beach',\n",
       " 'Colorado Springs',\n",
       " 'Raleigh',\n",
       " 'Miami',\n",
       " 'Virginia Beach',\n",
       " 'Omaha',\n",
       " 'Oakland',\n",
       " 'Minneapolis',\n",
       " 'Tulsa',\n",
       " 'Arlington',\n",
       " 'New Orleans',\n",
       " 'Wichita']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull Yelp.com's business links and store in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(city_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York\n",
      "Los Angeles\n",
      "Chicago\n",
      "Houston\n",
      "Phoenix\n",
      "Philadelphia\n",
      "San Antonio\n",
      "San Diego\n",
      "Dallas\n",
      "San Jose\n",
      "Austin\n",
      "Jacksonville\n",
      "San Francisco\n",
      "Columbus\n",
      "Indianapolis\n",
      "Fort Worth\n",
      "Charlotte\n",
      "Seattle\n",
      "Denver\n",
      "El Paso\n",
      "Washington, D.C\n",
      "Boston\n",
      "Detroit\n",
      "Nashville\n",
      "Memphis\n",
      "Portland\n",
      "Oklahoma City\n",
      "Las Vegas\n",
      "Louisville\n",
      "Baltimore\n",
      "Milwaukee\n",
      "Albuquerque\n",
      "Tucson\n",
      "Fresno\n",
      "Sacramento\n",
      "Mesa\n",
      "Kansas City\n",
      "Atlanta\n",
      "Long Beach\n",
      "Colorado Springs\n",
      "Raleigh\n",
      "Miami\n",
      "Virginia Beach\n",
      "Omaha\n",
      "Oakland\n",
      "Minneapolis\n",
      "Tulsa\n",
      "Arlington\n",
      "New Orleans\n",
      "Wichita\n"
     ]
    }
   ],
   "source": [
    "#Create empty page links list\n",
    "page_links = []\n",
    "\n",
    "for i, city in enumerate(city_list):\n",
    "    BASE_URL = 'https://www.yelp.com/search?find_desc=Restaurants&find_loc=' + city + '&start='\n",
    "    page_counts = [0,10,20,30]\n",
    "    for page_limit in page_counts:\n",
    "        response = requests.get(BASE_URL +  str(page_limit) + '&sortby=review_count',headers=HEADERS)\n",
    "        soup = BeautifulSoup(response.content, \"html5lib\")\n",
    "        links = soup.findAll(\"a\", { \"class\" : \"biz-name js-analytics-click\" })\n",
    "        for link in links:\n",
    "            page_links.append(link['href'])\n",
    "        time.sleep(5)\n",
    "    print(city)\n",
    "#     if i % 5 == 0:\n",
    "#         print(float(i/len(city_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir = \"/Users/Collier/Dropbox/Skills/Python/Projects/Culture/Cities_Bars_Yelp/restaurants/\"\n",
    "os.chdir(dir)\n",
    "page_links_save = pd.DataFrame(page_links)\n",
    "page_links_save.to_csv(dir+'restaurants_links.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2010"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(page_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop Through Yelp Links and Scrape Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've pulled links for about 1200 bars across 30 cities.  Now, we will scrape each Yelp page and pull the details for each business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_URL = 'http://www.yelp.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = ['Name','Rating','Reviews','Area','Address',\n",
    "'Takes Reservations','Delivery','Take-out',\n",
    "'Good For','Parking','Bike Parking',\n",
    "'Good for Kids','Good for Groups','Attire','Ambience',\n",
    "'Noise Level','Alcohol','Outdoor Seating','Wi-Fi',\n",
    "'Has TV','Dogs Allowed','Waiter Service','Caters',\n",
    "'Gender Neutral Restrooms','Price_Range','Price_Level']\n",
    "df_master = pd.DataFrame(columns=cols)\n",
    "df_master.reset_index(drop=True,inplace=True)\n",
    "df_master_all = df_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Area</th>\n",
       "      <th>Address</th>\n",
       "      <th>Takes Reservations</th>\n",
       "      <th>Delivery</th>\n",
       "      <th>Take-out</th>\n",
       "      <th>Good For</th>\n",
       "      <th>Parking</th>\n",
       "      <th>Bike Parking</th>\n",
       "      <th>Good for Kids</th>\n",
       "      <th>Good for Groups</th>\n",
       "      <th>Attire</th>\n",
       "      <th>Ambience</th>\n",
       "      <th>Noise Level</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Outdoor Seating</th>\n",
       "      <th>Wi-Fi</th>\n",
       "      <th>Has TV</th>\n",
       "      <th>Dogs Allowed</th>\n",
       "      <th>Waiter Service</th>\n",
       "      <th>Caters</th>\n",
       "      <th>Gender Neutral Restrooms</th>\n",
       "      <th>Price_Range</th>\n",
       "      <th>Price_Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Name, Rating, Reviews, Area, Address, Takes Reservations, Delivery, Take-out, Good For, Parking, Bike Parking, Good for Kids, Good for Groups, Attire, Ambience, Noise Level, Alcohol, Outdoor Seating, Wi-Fi, Has TV, Dogs Allowed, Waiter Service, Caters, Gender Neutral Restrooms, Price_Range, Price_Level]\n",
       "Index: []"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_master_all.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "link = page_links[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-30-af7fad3bdedd>, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-30-af7fad3bdedd>\"\u001b[0;36m, line \u001b[0;32m23\u001b[0m\n\u001b[0;31m    attr = []\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(BASE_URL + link, headers=HEADERS)\n",
    "soup = BeautifulSoup(response.content, \"html5lib\")\n",
    "#try to find name of establishment\n",
    "# try:\n",
    "#     name = soup.find(\"h1\", {\"class\":\"biz-page-title embossed-text-white\"}).contents[0].strip()\n",
    "# except:\n",
    "#     #if cannot find name, look at other name location, if cannot find that skip row\n",
    "# try:\n",
    "#     name = soup.find(\"h1\", {\"class\":\"biz-page-title embossed-text-white shortenough\"}).contents[0].strip()\n",
    "# except:\n",
    "#         continue\n",
    "feature_attr = soup.select_one('div.short-def-list').select('dt.attribute-key')\n",
    "feature_response = soup.select_one('div.short-def-list').select('dd')\n",
    "response = []\n",
    "#if we successfully scraped the features, loop through and pull text\n",
    "if feature_response:\n",
    "    for tag in feature_response:\n",
    "        text = tag.text.strip()\n",
    "        response.append(text)\n",
    "#if the features are missing, skip row\n",
    "else:\n",
    "#     continue\n",
    "attr = []\n",
    "#if we successfully scraped the features, loop through and pull text\n",
    "if feature_attr:\n",
    "    for tag in feature_attr:\n",
    "        text = tag.text.strip()\n",
    "        attr.append(text)\n",
    "#if the features are missing, skip row\n",
    "else:\n",
    "#     continue\n",
    "df_bus_info = pd.DataFrame({'VenueAttr_Desc':attr,'VenueAttr_Status':response})\n",
    "df_bus_info = pd.DataFrame(response,attr)\n",
    "df_bus_info = df_bus_info.T\n",
    "try:\n",
    "    pricing = soup.find(\"dd\", { \"class\" : \"nowrap price-description\" }).contents[0].strip()\n",
    "    pricing = pricing.replace(\"$\",\"\")\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    pricing_level = soup.find(\"span\", {\"class\":\"business-attribute price-range\"}).contents[0].strip()\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    ratings = soup.find('span', {\"class\" : \"review-count rating-qualifier\" }).contents[0].strip()\n",
    "    ratings = ratings.replace(\" reviews\",\"\")\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    stars = str(soup.find('div', {'class': 'biz-rating biz-rating-very-large clearfix'}).contents[1])\n",
    "    stars = stars[stars.find(\"title=\"):]\n",
    "    stars = stars[:stars.find(\"rating\")]\n",
    "    stars = stars[stars.find(\"=\"):]\n",
    "    stars = stars[:stars.find(\" star\")]\n",
    "    stars = re.sub('[^a-zA-Z0-9 \\n\\.]', '', stars)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    address = str(soup.find('strong',{'class':'street-address'}).contents[1])\n",
    "    address = address[:address.find(\"\\n    </address>\")]\n",
    "    address = re.sub(\"<address>\\n        \",\"\",address)\n",
    "    address = re.sub(\"<br/>\",\" \",address)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    area = soup.find('span',{'class':'neighborhood-str-list'}).contents[0]\n",
    "    area = re.sub(\"\\n            \",\"\",area)\n",
    "    area = re.sub(\"        \",\"\",area)\n",
    "except:\n",
    "    pass\n",
    "cols = ['Name','Rating','Reviews','Area','Address',\n",
    "'Takes Reservations','Parking','Bike Parking',\n",
    "'Good for Groups','Ambience','Noise Level',\n",
    "'Music','Good For Dancing','Alcohol','Happy Hour',\n",
    "'Best Nights','Smoking','Outdoor Seating','Wi-Fi',\n",
    "'Has TV','Waiter Service','Has Pool Table','Price_Range','Price_Level']\n",
    "df_master = pd.DataFrame(columns=cols,index=[0])\n",
    "df_master.reset_index(inplace=True)\n",
    "try:\n",
    "    df_master.loc[0, 'Name'] = name\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    df_master.loc[0, 'Rating'] = stars\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    df_master.loc[0, 'Reviews'] = ratings\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    df_master.loc[0, 'Area'] = area\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    df_master.loc[0, 'Address'] = address\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    df_master.loc[0, 'Takes Reservations'] = df_bus_info.loc[0,'Takes Reservations']\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    df_master.loc[0, 'Parking'] = df_bus_info.loc[0,'Parking']\n",
    "except:\n",
    "    pass\n",
    "try:    \n",
    "    df_master.loc[0, 'Bike Parking'] = df_bus_info.loc[0,'Bike Parking']\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    df_master.loc[0, 'Good for Groups'] = df_bus_info.loc[0,'Good for Groups']\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    df_master.loc[0, 'Ambience'] = df_bus_info.loc[0,'Ambience']\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    df_master.loc[0, 'Noise Level'] = df_bus_info.loc[0,'Noise Level']\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    df_master.loc[0, 'Music'] = df_bus_info.loc[0,'Music']\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    df_master.loc[0, 'Good For Dancing'] = df_bus_info.loc[0,'Good For Dancing']\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    df_master.loc[0, 'Alcohol'] = df_bus_info.loc[0,'Alcohol']\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    df_master.loc[0, 'Happy Hour'] = df_bus_info.loc[0,'Happy Hour']\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    df_master.loc[0, 'Best Nights'] = df_bus_info.loc[0,'Best Nights']\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    df_master.loc[0, 'Smoking'] = df_bus_info.loc[0,'Smoking']\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    df_master.loc[0, 'Outdoor Seating'] = df_bus_info.loc[0,'Outdoor Seating']\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    df_master.loc[0, 'Wi-Fi'] = df_bus_info.loc[0,'Wi-Fi']\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    df_master.loc[0, 'Has TV'] = df_bus_info.loc[0,'Has TV']\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    df_master.loc[0, 'Waiter_Service'] = df_bus_info.loc[0,'Waiter Service']\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    df_master.loc[0, 'Has Pool Table'] = df_bus_info.loc[0,'Has Pool Table']\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    df_master.loc[0, 'Price_Range'] = pricing\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    df_master.loc[0, 'Price_Level'] = pricing_level\n",
    "except:\n",
    "    pass\n",
    "df_master_all = df_master_all.append(df_master)\n",
    "time.sleep(5)\n",
    "if idx % 100 == 0:\n",
    "    print(float(idx/len(page_links)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-f15a4904b2f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_URL\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlink\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHEADERS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html5lib\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;31m#try to find name of establishment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Collier/miniconda3/envs/env1/lib/python3.5/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mParserRejectedMarkup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Collier/miniconda3/envs/env1/lib/python3.5/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m_feed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0;31m# Close out any unfinished strings and close all the open tags.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Collier/miniconda3/envs/env1/lib/python3.5/site-packages/bs4/builder/_html5lib.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mextra_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_specified_encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Set the character encoding detected by the tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Collier/miniconda3/envs/env1/lib/python3.5/site-packages/html5lib/html5parser.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, stream, encoding, parseMeta, useChardet)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \"\"\"\n\u001b[1;32m    223\u001b[0m         self._parse(stream, innerHTML=False, encoding=encoding,\n\u001b[0;32m--> 224\u001b[0;31m                     parseMeta=parseMeta, useChardet=useChardet)\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Collier/miniconda3/envs/env1/lib/python3.5/site-packages/html5lib/html5parser.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(self, stream, innerHTML, container, encoding, parseMeta, useChardet, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mReparseException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Collier/miniconda3/envs/env1/lib/python3.5/site-packages/html5lib/html5parser.py\u001b[0m in \u001b[0;36mmainLoop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mParseErrorToken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenTypes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ParseError\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalizedTokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0mnew_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mnew_token\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Collier/miniconda3/envs/env1/lib/python3.5/site-packages/html5lib/html5parser.py\u001b[0m in \u001b[0;36mnormalizedTokens\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnormalizedTokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalizeToken\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Collier/miniconda3/envs/env1/lib/python3.5/site-packages/html5lib/tokenizer.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Start processing. When EOF is reached self.state will return False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# instead of True and the loop will terminate.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokenTypes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ParseError\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Collier/miniconda3/envs/env1/lib/python3.5/site-packages/html5lib/tokenizer.py\u001b[0m in \u001b[0;36mdataState\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0;31m# emitted separately.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             self.tokenQueue.append({\"type\": tokenTypes[\"SpaceCharacters\"], \"data\":\n\u001b[0;32m--> 270\u001b[0;31m                                     data + self.stream.charsUntil(spaceCharacters, True)})\n\u001b[0m\u001b[1;32m    271\u001b[0m             \u001b[0;31m# No need to update lastFourChars here, since the first space will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;31m# have already been appended to lastFourChars and will have broken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Collier/miniconda3/envs/env1/lib/python3.5/site-packages/html5lib/inputstream.py\u001b[0m in \u001b[0;36mcharsUntil\u001b[0;34m(self, characters, opposite)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0;31m# Find the longest matching prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunkOffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0;31m# If nothing matched, and it wasn't because we ran out of chunk,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for idx, link in enumerate(page_links):\n",
    "#     if idx > 3:\n",
    "#         break\n",
    "#     else:\n",
    "        response = requests.get(BASE_URL + link, headers=HEADERS)\n",
    "        soup = BeautifulSoup(response.content, \"html5lib\")\n",
    "        #try to find name of establishment\n",
    "        try:\n",
    "            name = soup.find(\"h1\", {\"class\":\"biz-page-title embossed-text-white\"}).contents[0].strip()\n",
    "        except:\n",
    "            #if cannot find name, look at other name location, if cannot find that skip row\n",
    "            try:\n",
    "                name = soup.find(\"h1\", {\"class\":\"biz-page-title embossed-text-white shortenough\"}).contents[0].strip()\n",
    "            except:\n",
    "                continue\n",
    "        feature_attr = soup.select_one('div.short-def-list').select('dt.attribute-key')\n",
    "        feature_response = soup.select_one('div.short-def-list').select('dd')\n",
    "        response = []\n",
    "        #if we successfully scraped the features, loop through and pull text\n",
    "        if feature_response:\n",
    "            for tag in feature_response:\n",
    "                text = tag.text.strip()\n",
    "                response.append(text)\n",
    "        #if the features are missing, skip row\n",
    "        else:\n",
    "            continue\n",
    "        attr = []\n",
    "        #if we successfully scraped the features, loop through and pull text\n",
    "        if feature_attr:\n",
    "            for tag in feature_attr:\n",
    "                text = tag.text.strip()\n",
    "                attr.append(text)\n",
    "        #if the features are missing, skip row\n",
    "        else:\n",
    "            continue\n",
    "        df_bus_info = pd.DataFrame({'VenueAttr_Desc':attr,'VenueAttr_Status':response})\n",
    "        df_bus_info = pd.DataFrame(response,attr)\n",
    "        df_bus_info = df_bus_info.T\n",
    "        try:\n",
    "            pricing = soup.find(\"dd\", { \"class\" : \"nowrap price-description\" }).contents[0].strip()\n",
    "            pricing = pricing.replace(\"$\",\"\")\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            pricing_level = soup.find(\"span\", {\"class\":\"business-attribute price-range\"}).contents[0].strip()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            ratings = soup.find('span', {\"class\" : \"review-count rating-qualifier\" }).contents[0].strip()\n",
    "            ratings = ratings.replace(\" reviews\",\"\")\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            stars = str(soup.find('div', {'class': 'biz-rating biz-rating-very-large clearfix'}).contents[1])\n",
    "            stars = stars[stars.find(\"title=\"):]\n",
    "            stars = stars[:stars.find(\"rating\")]\n",
    "            stars = stars[stars.find(\"=\"):]\n",
    "            stars = stars[:stars.find(\" star\")]\n",
    "            stars = re.sub('[^a-zA-Z0-9 \\n\\.]', '', stars)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            address = str(soup.find('strong',{'class':'street-address'}).contents[1])\n",
    "            address = address[:address.find(\"\\n    </address>\")]\n",
    "            address = re.sub(\"<address>\\n        \",\"\",address)\n",
    "            address = re.sub(\"<br/>\",\" \",address)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            area = soup.find('span',{'class':'neighborhood-str-list'}).contents[0]\n",
    "            area = re.sub(\"\\n            \",\"\",area)\n",
    "            area = re.sub(\"        \",\"\",area)\n",
    "        except:\n",
    "            pass\n",
    "        cols = ['Name','Rating','Reviews','Area','Address',\n",
    "        'Takes Reservations','Parking','Bike Parking',\n",
    "        'Good for Groups','Ambience','Noise Level',\n",
    "        'Music','Good For Dancing','Alcohol','Happy Hour',\n",
    "        'Best Nights','Smoking','Outdoor Seating','Wi-Fi',\n",
    "        'Has TV','Waiter Service','Has Pool Table','Price_Range','Price_Level']\n",
    "        df_master = pd.DataFrame(columns=cols,index=[0])\n",
    "        df_master.reset_index(inplace=True)\n",
    "        try:\n",
    "            df_master.loc[0, 'Name'] = name\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            df_master.loc[0, 'Rating'] = stars\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            df_master.loc[0, 'Reviews'] = ratings\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            df_master.loc[0, 'Area'] = area\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            df_master.loc[0, 'Address'] = address\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            df_master.loc[0, 'Takes Reservations'] = df_bus_info.loc[0,'Takes Reservations']\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            df_master.loc[0, 'Parking'] = df_bus_info.loc[0,'Parking']\n",
    "        except:\n",
    "            pass\n",
    "        try:    \n",
    "            df_master.loc[0, 'Bike Parking'] = df_bus_info.loc[0,'Bike Parking']\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            df_master.loc[0, 'Good for Groups'] = df_bus_info.loc[0,'Good for Groups']\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            df_master.loc[0, 'Ambience'] = df_bus_info.loc[0,'Ambience']\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            df_master.loc[0, 'Noise Level'] = df_bus_info.loc[0,'Noise Level']\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            df_master.loc[0, 'Music'] = df_bus_info.loc[0,'Music']\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            df_master.loc[0, 'Good For Dancing'] = df_bus_info.loc[0,'Good For Dancing']\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            df_master.loc[0, 'Alcohol'] = df_bus_info.loc[0,'Alcohol']\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            df_master.loc[0, 'Happy Hour'] = df_bus_info.loc[0,'Happy Hour']\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            df_master.loc[0, 'Best Nights'] = df_bus_info.loc[0,'Best Nights']\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            df_master.loc[0, 'Smoking'] = df_bus_info.loc[0,'Smoking']\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            df_master.loc[0, 'Outdoor Seating'] = df_bus_info.loc[0,'Outdoor Seating']\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            df_master.loc[0, 'Wi-Fi'] = df_bus_info.loc[0,'Wi-Fi']\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            df_master.loc[0, 'Has TV'] = df_bus_info.loc[0,'Has TV']\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            df_master.loc[0, 'Waiter_Service'] = df_bus_info.loc[0,'Waiter Service']\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            df_master.loc[0, 'Has Pool Table'] = df_bus_info.loc[0,'Has Pool Table']\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            df_master.loc[0, 'Price_Range'] = pricing\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            df_master.loc[0, 'Price_Level'] = pricing_level\n",
    "        except:\n",
    "            pass\n",
    "        df_master_all = df_master_all.append(df_master)\n",
    "        time.sleep(5)\n",
    "        if idx % 100 == 0:\n",
    "            print(float(idx/len(page_links)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We've successfully pulled destails for 1055 of the 1216 business links.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 26)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_master_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir = \"/Users/Collier/Dropbox/Skills/Python/Projects/Culture/Cities_Bars_Yelp/\"\n",
    "os.chdir(dir)\n",
    "# page_links_save = pd.DataFrame(page_links)\n",
    "df_master_all.to_csv(dir+'yelp_data_bars.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:env1]",
   "language": "python",
   "name": "conda-env-env1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
